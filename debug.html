<!DOCTYPE html>
<!--
  Debug Page - è°ƒè¯•æ§åˆ¶å°
  
  åŠŸèƒ½ï¼š
  1. å®æ—¶æ˜¾ç¤ºMediaPipeæ£€æµ‹ç»“æœï¼ˆå›¾åƒ+éŸ³é¢‘ï¼‰
  2. LRUç¼“å­˜å¯è§†åŒ–ï¼ˆæœ€è¿‘20ä¸ªæ£€æµ‹ï¼‰
  3. VLMçŠ¶æ€ç›‘æ§ï¼ˆProcessing/Calls/Lock/Resultï¼‰
  4. RNæ¶ˆæ¯å†å²ï¼ˆæœ€è¿‘10æ¡ï¼‰
  5. å†³ç­–æ—¥å¿—ï¼ˆçª—å£è§¦å‘/çŠ¶æ€å˜åŒ–ï¼‰
  
  æœ€æ–°æ”¹åŠ¨ (2025-10-08):
  - é›†æˆRNæ¶ˆæ¯æ¡¥æ¥ï¼ˆå®Œæ•´æ ¼å¼ï¼‰
  - RNæ¶ˆæ¯é¢æ¿æ˜¾ç¤ºï¼šæ—¶é—´ã€sourceã€stateã€textã€metadata
  - é¢œè‰²ç¼–ç ï¼šstate=ç»¿è‰²ã€vlm=é»„è‰²ã€emotion=ç´«è‰²
-->
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Mewt Debug Console</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Courier New', monospace;
      background: #1a1a1a;
      color: #00ff00;
      overflow: hidden;
    }

    #container {
      display: flex;
      flex-direction: column;
      height: 100vh;
      gap: 8px;
      padding: 8px;
      overflow-y: auto;
    }

    #video-preview {
      height: 120px;
      min-height: 120px;
      position: relative;
      background: #000;
      border: 2px solid #00ff00;
      border-radius: 5px;
      overflow: hidden;
    }

    #webcam {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    #status-panel {
      display: flex;
      gap: 8px;
      min-height: 100px;
    }

    #status-panel .panel {
      flex: 1;
    }

    #debug-panels {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 8px;
    }

    .panel {
      background: #0a0a0a;
      border: 1px solid #00ff00;
      border-radius: 5px;
      padding: 10px;
      overflow-y: auto;
    }

    .panel h3 {
      color: #ffff00;
      margin-bottom: 10px;
      font-size: 14px;
      border-bottom: 1px solid #00ff00;
      padding-bottom: 5px;
    }

    .status-item {
      margin: 5px 0;
      font-size: 12px;
    }

    .status-label {
      color: #888;
      display: inline-block;
      width: 80px;
    }

    .status-value {
      color: #00ff00;
      font-weight: bold;
    }

    .detection-item {
      display: flex;
      justify-content: space-between;
      margin: 3px 0;
      font-size: 11px;
    }

    .pass {
      color: #00ff00;
    }

    .fail {
      color: #ff4444;
    }

    .lru-item {
      font-size: 10px;
      margin: 2px 0;
      padding: 2px 5px;
      background: #1a1a1a;
      border-left: 2px solid #00ff00;
    }

    .lru-cat {
      border-left-color: #ffff00;
    }

    #log-panel {
      grid-column: 1 / -1;
    }

    #vlm-result {
      word-wrap: break-word;
      white-space: pre-wrap;
    }

    .log-entry {
      font-size: 11px;
      margin: 2px 0;
      padding: 2px 5px;
      border-left: 2px solid #00ff00;
    }

    .log-time {
      color: #888;
    }

    .log-state {
      color: #ffff00;
    }

    .log-decision {
      color: #00ff00;
    }

    .state-badge {
      display: inline-block;
      padding: 5px 10px;
      border-radius: 3px;
      font-weight: bold;
      font-size: 18px;
    }

    .state-idle {
      background: #444;
      color: #aaa;
    }

    .state-cat_visual {
      background: #4444ff;
      color: #fff;
    }

    .state-cat_audio {
      background: #ff8800;
      color: #fff;
    }

    .state-cat_both {
      background: #00ff00;
      color: #000;
    }

    .progress-bar {
      width: 100%;
      height: 10px;
      background: #333;
      border-radius: 5px;
      overflow: hidden;
      margin: 5px 0;
    }

    .progress-fill {
      height: 100%;
      background: linear-gradient(90deg, #00ff00, #ffff00);
      transition: width 0.1s linear;
    }

    #loading {
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      color: #00ff00;
      font-size: 24px;
      text-align: center;
      z-index: 1000;
    }

    .hidden {
      display: none;
    }

    ::-webkit-scrollbar {
      width: 8px;
    }

    ::-webkit-scrollbar-track {
      background: #1a1a1a;
    }

    ::-webkit-scrollbar-thumb {
      background: #00ff00;
      border-radius: 4px;
    }
  </style>
</head>
<body>
  <div id="loading">Loading models...<br/><small>This may take a moment</small></div>
  
  <div id="container" class="hidden">
    <!-- Video Preview -->
    <div id="video-preview">
      <video id="webcam" autoplay playsinline muted></video>
    </div>

    <!-- Status Panel -->
    <div id="status-panel">
      <div class="panel">
        <h3>ğŸ¯ CURRENT STATE</h3>
        <div class="status-item">
          <span class="state-badge" id="state-badge">IDLE</span>
        </div>
        <div class="status-item">
          <span class="status-label">Focusing:</span>
          <span class="status-value" id="focusing-status">NO</span>
        </div>
        <div class="status-item">
          <span class="status-label">Response:</span>
          <span class="status-value" id="state-response">è§‚å¯Ÿä¸­...</span>
        </div>
      </div>

      <div class="panel">
        <h3>â±ï¸ WINDOW STATUS</h3>
        <div class="status-item">
          <span class="status-label">Progress:</span>
          <div class="progress-bar">
            <div class="progress-fill" id="window-progress"></div>
          </div>
        </div>
        <div class="status-item">
          <span class="status-label">Image:</span>
          <span class="status-value" id="image-count">0</span>
        </div>
        <div class="status-item">
          <span class="status-label">Audio:</span>
          <span class="status-value" id="audio-count">0</span>
        </div>
      </div>
    </div>

    <!-- Debug Panels -->
    <div id="debug-panels">
      <div class="panel">
        <h3>ğŸ“· IMAGE DETECTIONS</h3>
        <div id="image-detections">Waiting...</div>
      </div>

      <div class="panel">
        <h3>ğŸ”Š AUDIO DETECTIONS</h3>
        <div id="audio-detections">Waiting...</div>
      </div>

      <div class="panel">
        <h3>ğŸ’¾ LRU CACHE</h3>
        <div class="status-item">
          <span class="status-label">Size:</span>
          <span class="status-value" id="lru-size">0/20</span>
        </div>
        <div class="status-item">
          <span class="status-label">Recent 10:</span>
          <span class="status-value" id="lru-cat-count">0 cats</span>
        </div>
        <div id="lru-items">Empty</div>
      </div>

      <div class="panel">
        <h3>ğŸ“¡ VLM STATUS</h3>
        <div class="status-item">
          <span class="status-label">Processing:</span>
          <span class="status-value" id="vlm-processing">NO</span>
        </div>
        <div class="status-item">
          <span class="status-label">Calls:</span>
          <span class="status-value" id="vlm-calls">0/3</span>
        </div>
        <div class="status-item">
          <span class="status-label">Last Call:</span>
          <span class="status-value" id="vlm-last-call">Never</span>
        </div>
        <div class="status-item">
          <span class="status-label">Lock:</span>
          <span class="status-value" id="vlm-lock">Unlocked</span>
        </div>
        <div class="status-item" style="margin-top: 10px;">
          <span class="status-label" style="width: 100%; color: #ffff00;">Last Result:</span>
          <div id="vlm-result" style="color: #00ff00; font-size: 11px; margin-top: 5px; padding: 5px; background: #1a1a1a; border-left: 2px solid #00ff00;">-</div>
        </div>
      </div>

      <div class="panel">
        <h3>ğŸ”¼ ä¸Šè¡Œæ¶ˆæ¯ (RNâ†’WebView)</h3>
        <div class="status-item">
          <span class="status-label">Count:</span>
          <span class="status-value" id="upstream-count">0</span>
        </div>
        <div id="upstream-messages" style="font-size: 9px;">æ— æ¶ˆæ¯</div>
      </div>

      <div class="panel">
        <h3>ğŸ”½ ä¸‹è¡Œæ¶ˆæ¯ (WebViewâ†’RN)</h3>
        <div class="status-item">
          <span class="status-label">Count:</span>
          <span class="status-value" id="downstream-count">0</span>
        </div>
        <div id="downstream-messages" style="font-size: 9px;">æ— æ¶ˆæ¯</div>
      </div>

      <div class="panel" id="log-panel">
        <h3>ğŸ“ DECISION LOG</h3>
        <div id="log-entries"></div>
      </div>
    </div>
  </div>

  <script type="module">
    import {
      ImageClassifier,
      FilesetResolver as VisionFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8";
    
    import {
      AudioClassifier,
      FilesetResolver as AudioFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8";
    
    import Mewt from "./mewt.js";
    
    // Import VLM manager
    import { VLMChannel } from "./vlm-manager.js";
    
    // Import RN message receiver (ç»Ÿä¸€æ¶ˆæ¯å¤„ç†)
    import rnReceiver from "./rn-message-receiver.js";
    
    // RNæ¶ˆæ¯è®°å½•ï¼ˆdebugæ¨¡å¼ï¼‰ - åŒå‘æ¶ˆæ¯
    const rnMessages = {
      sent: [],      // å‘é€åˆ°RNçš„æ¶ˆæ¯
      received: []   // ä»RNæ”¶åˆ°çš„æ¶ˆæ¯
    };
    
    window.sendToRN = function(text, source, state, metadata = {}) {
      const message = {
        type: 'text_update',
        text,
        source,
        state,
        timestamp: Date.now(),
        metadata,
        direction: 'sent'
      };
      
      // è®°å½•æ¶ˆæ¯ï¼ˆç”¨äºdebugæ˜¾ç¤ºï¼‰
      rnMessages.sent.push(message);
      if (rnMessages.sent.length > 50) rnMessages.sent.shift();
      
      // å®é™…å‘é€ï¼ˆå¦‚æœåœ¨RN WebViewä¸­ï¼‰
      if (window.ReactNativeWebView) {
        window.ReactNativeWebView.postMessage(JSON.stringify(message));
      }
      console.log('[â†’ RN]', message);
      
      // æ›´æ–°æ˜¾ç¤º
      updateRNPanel();
    };

    // DOM elements
    const video = document.getElementById("webcam");
    const loadingElement = document.getElementById("loading");
    const container = document.getElementById("container");
    
    // Classifiers
    let imageClassifier;
    let audioClassifier;
    let audioContext;
    
    // Mewt instance
    const mewt = new Mewt();
    
    // Initialize VLM Vision Channel
    const vlmVision = new VLMChannel('vision', {
      enabled: true,
      minInterval: 15000,
      maxPerMinute: 3
    });
    
    // ==================== DeepMewtçŠ¶æ€ç®¡ç† ====================
    // æ·»åŠ DeepMewtçŠ¶æ€ç®¡ç†ï¼šç»´æŠ¤å…¨å±€çŠ¶æ€ï¼Œæ¥æ”¶RNæ§åˆ¶æ¶ˆæ¯
    // æ”¹åŠ¨åŸå› ï¼šæ”¯æŒRNåŠ¨æ€åˆ‡æ¢DeepMewtæ¨¡å¼ï¼Œä¸ºåç»­audio+videoå¢å¼ºåšå‡†å¤‡
    
    // å…¨å±€DeepMewtçŠ¶æ€
    window.DEEP_MEWT_ENABLED = false;
    
    // ==================== ä½¿ç”¨ç»Ÿä¸€çš„ RN æ¶ˆæ¯æ¥æ”¶å™¨ ====================
    // æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨ - DRYåŸåˆ™ï¼Œé¿å…é‡å¤ä»£ç 
    rnReceiver
      .on('deep_mewt_toggle', (data) => {
        // è®°å½•æ”¶åˆ°çš„æ¶ˆæ¯
        const receivedMsg = {
          ...data,
          direction: 'received',
          receivedAt: Date.now()
        };
        rnMessages.received.push(receivedMsg);
        if (rnMessages.received.length > 50) rnMessages.received.shift();
        
        addLog(`ğŸ“¥ æ”¶åˆ°RNæ¶ˆæ¯: ${data.type}`);
        updateRNPanel();
        
        // æ›´æ–°å…¨å±€çŠ¶æ€
        const previousState = window.DEEP_MEWT_ENABLED;
        window.DEEP_MEWT_ENABLED = data.enabled;
        
        console.log(`ğŸ§  DeepMewtçŠ¶æ€å˜æ›´: ${previousState ? 'ON' : 'OFF'} â†’ ${data.enabled ? 'ON' : 'OFF'}`);
        addLog(`ğŸ§  DeepMewt: ${data.enabled ? 'ON' : 'OFF'}`);
        
        // å‘é€çŠ¶æ€ç¡®è®¤æ¶ˆæ¯åˆ°RN
        sendToRN(
          `DeepMewtæ¨¡å¼${data.enabled ? 'å·²å¼€å¯' : 'å·²å…³é—­'}`,
          'system',
          'idle',
          { 
            deepMewtEnabled: data.enabled,
            previousState: previousState,
            timestamp: Date.now()
          }
        );
      })
      .on('take_photo', (data) => {
        // è®°å½•æ”¶åˆ°çš„æ¶ˆæ¯
        const receivedMsg = {
          ...data,
          direction: 'received',
          receivedAt: Date.now()
        };
        rnMessages.received.push(receivedMsg);
        if (rnMessages.received.length > 50) rnMessages.received.shift();
        
        console.log('[Photo] æ”¶åˆ°æ‹ç…§è¯·æ±‚');
        addLog(`ğŸ“¸ æ”¶åˆ°æ‹ç…§è¯·æ±‚ (timestamp: ${data.timestamp})`);
        updateRNPanel();
        
        // è·å–å½“å‰å¸§æˆªå›¾
        const imageData = getCurrentFrame();
        const imageSizeKB = (imageData.length / 1024).toFixed(2);
        
        addLog(`ğŸ“¸ æˆªå›¾å®Œæˆï¼Œå¤§å°: ${imageSizeKB} KB`);
        
        // å‘é€æˆªå›¾å›RN
        sendToRN(
          'ç…§ç‰‡å·²æ‹æ‘„',
          'photo',
          stableState,
          {
            imageData: imageData,
            timestamp: Date.now(),
            videoSize: {
              width: video.videoWidth,
              height: video.videoHeight
            }
          }
        );
        
        addLog(`ğŸ“¸ æˆªå›¾å·²å‘é€åˆ°RN (${video.videoWidth}x${video.videoHeight})`);
      });
    
    // Window timer tracking
    let windowStartTime = Date.now();
    const WINDOW_INTERVAL = 1000;
    
    // Log management
    const maxLogEntries = 100;
    const logEntries = [];
    
    // Latest predictions (for display only)
    let latestPredictions = {
      image: [],
      audio: []
    };
    
    // State stability mechanism
    let stableState = 'idle';
    let pendingState = 'idle';
    let stateChangeTime = 0;
    let lastStableState = 'idle';
    const STATE_DEBOUNCE_MS = 2000; // 2 seconds
    
    // Response deduplication
    let lastResponse = '';
    
    // Helper: Get current video frame as base64
    const getCurrentFrame = () => {
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx = canvas.getContext('2d');
      ctx.drawImage(video, 0, 0);
      return canvas.toDataURL('image/jpeg', 0.8);
    };

    // Start camera immediately
    const startCamera = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: { ideal: "environment" } }
        });
        video.srcObject = stream;
        
        return new Promise((resolve) => {
          video.addEventListener("playing", () => {
            setTimeout(() => {
              if (video.videoWidth > 0 && video.videoHeight > 0) {
                console.log(`âœ“ Camera ready: ${video.videoWidth}x${video.videoHeight}`);
                addLog(`Camera initialized: ${video.videoWidth}x${video.videoHeight}`);
                resolve(true);
              } else {
                console.error("Video playing but dimensions are 0");
                resolve(false);
              }
            }, 100);
          });
        });
      } catch (err) {
        console.error("Error accessing rear camera:", err);
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ video: true });
          video.srcObject = stream;
          
          return new Promise((resolve) => {
            video.addEventListener("playing", () => {
              setTimeout(() => {
                if (video.videoWidth > 0 && video.videoHeight > 0) {
                  console.log(`âœ“ Camera ready (fallback): ${video.videoWidth}x${video.videoHeight}`);
                  addLog(`Camera initialized (fallback): ${video.videoWidth}x${video.videoHeight}`);
                  resolve(true);
                } else {
                  resolve(false);
                }
              }, 100);
            });
          });
        } catch (fallbackErr) {
          console.error("Cannot access camera:", fallbackErr);
          addLog("âœ— Cannot access camera");
          loadingElement.innerHTML = "Error accessing camera: " + fallbackErr.message;
          return false;
        }
      }
    };

    // Load models
    const loadModels = async () => {
      try {
        loadingElement.innerHTML = "Loading image classifier...";
        
        // Load vision
        const vision = await VisionFilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8/wasm"
        );
        imageClassifier = await ImageClassifier.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite`
          },
          maxResults: 5,
          runningMode: "VIDEO"
        });

        loadingElement.innerHTML = "Loading audio classifier...";
        
        // Load audio
        const audio = await AudioFilesetResolver.forAudioTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8/wasm"
        );
        audioClassifier = await AudioClassifier.createFromOptions(audio, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite"
          },
          maxResults: 5
        });

        return true;
      } catch (error) {
        console.error("Error loading models:", error);
        loadingElement.innerHTML = "Error: " + error.message;
        return false;
      }
    };

    // Initialize: camera + models in parallel
    const initialize = async () => {
      loadingElement.innerHTML = "Starting camera...";
      
      // Start camera and load models in parallel
      const [cameraReady, modelsReady] = await Promise.all([
        startCamera(),
        loadModels()
      ]);
      
      if (!cameraReady || !modelsReady) {
        if (!modelsReady) {
          loadingElement.innerHTML = "Error loading models";
        }
        return;
      }
      
      loadingElement.classList.add("hidden");
      container.classList.remove("hidden");
      startClassification();
    };

    // Start classification (camera already started in initialize)
    const startClassification = async () => {
      // Start image classification (video already playing)
      predictWebcam();
      
      // Start audio classification
      startAudioClassification();
      
      // Start debug update loop
      startDebugUpdates();
    };

    // Predict from webcam
    const predictWebcam = async () => {
      if (!imageClassifier || !video.videoWidth || !video.videoHeight) {
        window.requestAnimationFrame(predictWebcam);
        return;
      }

      try {
        const result = imageClassifier.classifyForVideo(video, performance.now());
        if (result.classifications.length > 0) {
          // Save for display
          latestPredictions.image = result.classifications[0].categories;
          
          // Send to mewt
          const predictions = result.classifications[0].categories.map(cat => ({
            class: cat.categoryName,
            score: cat.score
          }));
          mewt.addImageResult(predictions);
        }
      } catch (error) {
        console.error("Image classification error:", error);
      }

      window.requestAnimationFrame(predictWebcam);
    };

    // Audio classification
    const startAudioClassification = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);
        const scriptNode = audioContext.createScriptProcessor(16384, 1, 1);

        scriptNode.onaudioprocess = function (event) {
          try {
            if (!audioClassifier) return;
            const inputData = event.inputBuffer.getChannelData(0);
            const result = audioClassifier.classify(inputData);
            const categories = result[0].classifications[0].categories;
            
            // Save for display
            latestPredictions.audio = categories;
            
            // Send to mewt
            const predictions = categories.map(cat => ({
              class: cat.categoryName,
              score: cat.score
            }));
            
            mewt.addAudioResult(predictions, inputData);
          } catch (error) {
            console.error("Audio classification error:", error);
          }
        };

        source.connect(scriptNode);
        scriptNode.connect(audioContext.destination);
      } catch (error) {
        console.error("Cannot access microphone:", error);
      }
    };

    // Debug update loop
    const startDebugUpdates = () => {
      setInterval(() => {
        updateDebugPanels();
        updateVLMStatus();
        updateWindowProgress();
      }, 100);
    };
    
    // Update VLM status panel
    const updateVLMStatus = () => {
      const now = Date.now();
      
      // Processing status
      document.getElementById("vlm-processing").textContent = 
        vlmVision.isProcessing ? "YES ğŸ”„" : "NO";
      document.getElementById("vlm-processing").style.color = 
        vlmVision.isProcessing ? "#ffff00" : "#00ff00";
      
      // Call count
      const callCount = vlmVision.rateLimiter.callsInLastMinute.length;
      document.getElementById("vlm-calls").textContent = `${callCount}/3`;
      document.getElementById("vlm-calls").style.color = 
        callCount >= 3 ? "#ff4444" : "#00ff00";
      
      // Last call time
      if (vlmVision.rateLimiter.lastCallTime > 0) {
        const secondsAgo = Math.floor((now - vlmVision.rateLimiter.lastCallTime) / 1000);
        document.getElementById("vlm-last-call").textContent = 
          secondsAgo === 0 ? "Just now" : `${secondsAgo}s ago`;
      } else {
        document.getElementById("vlm-last-call").textContent = "Never";
      }
      
      // Lock status
      const lockRemaining = Math.max(0, vlmVision.lock.until - now);
      if (lockRemaining > 0) {
        const secondsLeft = Math.ceil(lockRemaining / 1000);
        document.getElementById("vlm-lock").textContent = `ğŸ”’ ${secondsLeft}s`;
        document.getElementById("vlm-lock").style.color = "#ffff00";
      } else {
        document.getElementById("vlm-lock").textContent = "Unlocked";
        document.getElementById("vlm-lock").style.color = "#888";
      }
      
      // Last result
      const vlmText = vlmVision.getText();
      if (vlmText) {
        document.getElementById("vlm-result").textContent = vlmText;
        document.getElementById("vlm-result").style.borderLeftColor = "#ffff00";
      } else if (vlmVision.lock.text) {
        // Expired lock, show grayed out
        document.getElementById("vlm-result").textContent = 
          `[å·²è¿‡æœŸ] ${vlmVision.lock.text}`;
        document.getElementById("vlm-result").style.borderLeftColor = "#444";
      } else {
        document.getElementById("vlm-result").textContent = "-";
        document.getElementById("vlm-result").style.borderLeftColor = "#00ff00";
      }
    };

    // Update debug panels
    const updateDebugPanels = () => {
      const context = mewt.getFullContext();
      
      // Enhance visual detection with LRU trust
      let hasVisual = mewt.hasVisualCat();
      if (!hasVisual) {
        const recent10 = context.image_lru.recent(10);
        const catCount = recent10.filter(item => item.isCat).length;
        if (catCount > 0) {
          hasVisual = true; // Trust LRU history
        }
      }
      
      const hasAudio = mewt.hasCatSound();
      const rawState = mewt.determineState(hasVisual, hasAudio);
      
      // Apply state debouncing
      const now = Date.now();
      if (rawState !== pendingState) {
        pendingState = rawState;
        stateChangeTime = now;
      }
      
      // Only update stable state if pending state persists for 2 seconds
      if (now - stateChangeTime >= STATE_DEBOUNCE_MS) {
        stableState = pendingState;
        
        // Detect state change
        if (stableState !== lastStableState) {
          handleStateChange(stableState, lastStableState);
          lastStableState = stableState;
        }
      }
      
      const state = stableState;
      
      const stateBadge = document.getElementById("state-badge");
      stateBadge.textContent = state.toUpperCase();
      stateBadge.className = `state-badge state-${state}`;
      
      document.getElementById("focusing-status").textContent = 
        context.is_now_focusing_cat ? "YES âœ“" : "NO";
      
      // Get text from VLM (if locked) or use default
      const vlmText = vlmVision.getText();
      
      const stateResponses = {
        'idle': 'è§‚å¯Ÿä¸­...',
        'cat_visual': 'é‚£é‡Œæœ‰åªå°çŒ«',
        'cat_audio': 'è¯¶ï¼Ÿæˆ‘å¥½åƒå¬åˆ°å°çŒ«å«äº†',
        'cat_both': 'å“¦ï¼æ˜¯ä¸ªå°çŒ«'
      };
      
      // Apply response deduplication (VLM text has priority)
      const newResponse = vlmText || stateResponses[state];
      if (newResponse !== lastResponse) {
        document.getElementById("state-response").textContent = newResponse;
        lastResponse = newResponse;
        
        // Send to RN with complete format
        const source = vlmText ? 'vlm' : 'state';
        const metadata = {
          hasCat: state !== 'idle',
          hasVisual: hasVisual,
          hasAudio: hasAudio
        };
        
        // Add VLM-specific metadata
        if (vlmText) {
          const vlmData = vlmVision.getLockData();
          metadata.confidence = vlmData?.confidence || 0.9;
          metadata.vlmLocked = true;
        }
        
        sendToRN(newResponse, source, state, metadata);
      }
      
      // Update image detections
      const imageDiv = document.getElementById("image-detections");
      if (latestPredictions.image.length > 0) {
        const threshold = 0.3;
        imageDiv.innerHTML = latestPredictions.image
          .slice(0, 5)
          .map(item => {
            const pass = item.categoryName.toLowerCase().includes('cat') && item.score > threshold;
            const symbol = pass ? 'âœ“' : 'âœ—';
            const className = pass ? 'pass' : 'fail';
            return `<div class="detection-item ${className}">
              <span>${item.categoryName}</span>
              <span>${item.score.toFixed(3)} ${symbol}</span>
            </div>`;
          }).join('');
      } else {
        imageDiv.innerHTML = '<div class="detection-item">No data</div>';
      }
      
      // Update audio detections
      const audioDiv = document.getElementById("audio-detections");
      if (latestPredictions.audio.length > 0) {
        const threshold = 0.2;
        audioDiv.innerHTML = latestPredictions.audio
          .slice(0, 5)
          .map(item => {
            const pass = (item.categoryName.toLowerCase().includes('cat') || 
                         item.categoryName.toLowerCase().includes('meow')) && item.score > threshold;
            const symbol = pass ? 'âœ“' : 'âœ—';
            const className = pass ? 'pass' : 'fail';
            return `<div class="detection-item ${className}">
              <span>${item.categoryName}</span>
              <span>${item.score.toFixed(3)} ${symbol}</span>
            </div>`;
          }).join('');
      } else {
        audioDiv.innerHTML = '<div class="detection-item">No data</div>';
      }
      
      // Update window counts
      document.getElementById("image-count").textContent = 
        latestPredictions.image.length;
      document.getElementById("audio-count").textContent = 
        latestPredictions.audio.length;
      
      // Update LRU cache
      const lruSize = context.image_lru.size();
      document.getElementById("lru-size").textContent = `${lruSize}/20`;
      
      const recent10 = context.image_lru.recent(10);
      const catCount = recent10.filter(item => item.isCat).length;
      document.getElementById("lru-cat-count").textContent = `${catCount} cats`;
      
      const lruDiv = document.getElementById("lru-items");
      if (lruSize > 0) {
        const allValues = context.image_lru.getAllValues();
        lruDiv.innerHTML = allValues.slice(-10).reverse()
          .map((item, idx) => {
            const className = item.isCat ? 'lru-item lru-cat' : 'lru-item';
            const time = new Date(item.timestamp).toLocaleTimeString();
            return `<div class="${className}">
              [${idx === 0 ? 'NEW' : `-${idx}`}] ${item.class} (${item.score.toFixed(2)}) - ${time}
            </div>`;
          }).join('');
      } else {
        lruDiv.innerHTML = '<div class="lru-item">Empty</div>';
      }
    };

    // Update window progress
    let lastWindowLog = 0;
    const updateWindowProgress = () => {
      const elapsed = Date.now() - windowStartTime;
      const progress = Math.min((elapsed / WINDOW_INTERVAL) * 100, 100);
      document.getElementById("window-progress").style.width = progress + '%';
      
      // Log window trigger
      if (progress >= 100) {
        windowStartTime = Date.now();
        
        // Log only if enough time has passed
        if (Date.now() - lastWindowLog > 900) {
          const context = mewt.getFullContext();
          const hasVisual = mewt.hasVisualCat();
          const hasAudio = mewt.hasCatSound();
          const state = mewt.determineState(hasVisual, hasAudio);
          
          addLog(`Window triggered â†’ State: ${state}`);
          
          if (state !== 'idle') {
            addLog(`Detection: Visual=${hasVisual ? 'âœ“' : 'âœ—'} Audio=${hasAudio ? 'âœ“' : 'âœ—'}`);
          }
          
          const recent10 = context.image_lru.recent(10);
          const catCount = recent10.filter(item => item.isCat).length;
          if (state === 'idle' && catCount > 0) {
            addLog(`LRU Trust: ${catCount}/10 cats â†’ Maintain focus`);
          }
          
          addLog(`Decision: Focusing=${context.is_now_focusing_cat ? 'YES' : 'NO'}`);
          
          lastWindowLog = Date.now();
        }
      }
    };

    // Update RN message panels - ä¸Šè¡Œ/ä¸‹è¡Œåˆ†å¼€æ˜¾ç¤º
    const updateRNPanel = () => {
      // ä¸Šè¡Œæ¶ˆæ¯ (RN â†’ WebView)
      document.getElementById("upstream-count").textContent = rnMessages.received.length;
      const upstreamDiv = document.getElementById("upstream-messages");
      
      if (rnMessages.received.length > 0) {
        upstreamDiv.innerHTML = rnMessages.received.slice(-5).reverse()
          .map(msg => {
            const time = new Date(msg.receivedAt).toLocaleTimeString();
            return `<div class="lru-item" style="border-left-color: #4444ff;">
              <div style="color: #888;">[${time}] ${msg.type}</div>
              ${msg.enabled !== undefined ? 
                `<div style="color: #4444ff; margin-top: 2px;">enabled: ${msg.enabled}</div>` : ''}
              ${msg.timestamp ? 
                `<div style="color: #666; margin-top: 2px; font-size: 8px;">ts: ${msg.timestamp}</div>` : ''}
            </div>`;
          }).join('');
      } else {
        upstreamDiv.innerHTML = '<div style="color: #666;">ç­‰å¾…RNæ¶ˆæ¯...</div>';
      }
      
      // ä¸‹è¡Œæ¶ˆæ¯ (WebView â†’ RN)
      document.getElementById("downstream-count").textContent = rnMessages.sent.length;
      const downstreamDiv = document.getElementById("downstream-messages");
      
      if (rnMessages.sent.length > 0) {
        downstreamDiv.innerHTML = rnMessages.sent.slice(-5).reverse()
          .map(msg => {
            const time = new Date(msg.timestamp).toLocaleTimeString();
            
            // source é¢œè‰²
            const sourceColor = msg.source === 'vlm' ? '#ffff00' : 
                              msg.source === 'emotion' ? '#ff00ff' : 
                              msg.source === 'photo' ? '#ff8800' : 
                              msg.source === 'system' ? '#4444ff' : '#00ff00';
            
            return `<div class="lru-item" style="border-left-color: ${sourceColor};">
              <div style="color: #888;">[${time}] ${msg.source}</div>
              ${msg.text ? `<div style="color: ${sourceColor}; margin-top: 2px;">${msg.text}</div>` : ''}
              ${msg.metadata?.imageData ? 
                `<div style="color: #666; margin-top: 2px; font-size: 8px;">ğŸ“· ${(msg.metadata.imageData.length/1024).toFixed(1)}KB</div>` : ''}
            </div>`;
          }).join('');
      } else {
        downstreamDiv.innerHTML = '<div style="color: #666;">æ— è¾“å‡º</div>';
      }
    };

    // Add log entry
    const addLog = (message) => {
      const time = new Date().toLocaleTimeString();
      logEntries.push({ time, message });
      
      if (logEntries.length > maxLogEntries) {
        logEntries.shift();
      }
      
      const logDiv = document.getElementById("log-entries");
      logDiv.innerHTML = logEntries.slice(-30).reverse()
        .map(entry => `<div class="log-entry">
          <span class="log-time">[${entry.time}]</span> ${entry.message}
        </div>`).join('');
    };

    // Handle state changes - trigger VLM when appropriate
    const handleStateChange = async (newState, oldState) => {
      console.log(`[State Change] ${oldState} â†’ ${newState}`);
      addLog(`State Change: ${oldState} â†’ ${newState}`);
      
      // Trigger VLM on transitions involving cat_visual or cat_both
      const hasVisual = newState === 'cat_visual' || newState === 'cat_both';
      const hadVisual = oldState === 'cat_visual' || oldState === 'cat_both';
      
      if (hasVisual && !hadVisual) {
        // Started seeing cat - call VLM
        console.log('[VLM Trigger] æ£€æµ‹åˆ°çŒ«ï¼Œè°ƒç”¨VLMç¡®è®¤');
        addLog('ğŸ” VLM: æ£€æµ‹åˆ°çŒ«ï¼Œæ­£åœ¨ç¡®è®¤...');
        const frame = getCurrentFrame();
        const result = await vlmVision.analyze({ image: frame });
        if (result) {
          addLog(`âœ“ VLM: ${result.text}`);
        }
      } else if (!hasVisual && hadVisual) {
        // Lost cat - call VLM to confirm
        console.log('[VLM Trigger] ä¸¢å¤±çŒ«ï¼Œè°ƒç”¨VLMç¡®è®¤');
        addLog('ğŸ” VLM: ä¸¢å¤±çŒ«ï¼Œæ­£åœ¨ç¡®è®¤...');
        const frame = getCurrentFrame();
        const result = await vlmVision.analyze({ image: frame });
        if (result) {
          addLog(`âœ“ VLM: ${result.text}`);
        }
      }
    };
    
    // Initialize
    window.addEventListener("DOMContentLoaded", initialize);
  </script>
</body>
</html>
