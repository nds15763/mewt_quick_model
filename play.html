<!DOCTYPE html>
<!--
  Play Page - 音视频检测主页面
  
  功能：
  1. MediaPipe实时图像+音频检测
  2. Mewt四状态分类（idle/cat_visual/cat_audio/cat_both）
  3. VLM详细分析（Kimi视觉模型）
  4. RN通讯：发送检测结果文案
  
  最新改动 (2025-10-08):
  - 集成RN消息桥接（rn-bridge.js）
  - 发送完整消息格式：{type, text, source, state, timestamp, metadata}
  - 统一发送逻辑：状态变化发state，VLM返回发vlm
-->
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MediaPipe Audio & Image Classification with Mewt</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      background-color: #000;
      font-family: Arial, sans-serif;
    }

    #container {
      position: relative;
      width: 100vw;
      height: 100vh;
    }

    #webcam {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    #predictionPanel {
      position: absolute;
      bottom: 20px;
      left: 20px;
      background-color: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 15px;
      border-radius: 5px;
      z-index: 10;
      min-width: 300px;
      max-width: 500px;
    }

    #predictionPanel h3 {
      margin: 0 0 10px 0;
      font-size: 18px;
    }

    #rawPredictions {
      margin-bottom: 15px;
      font-size: 14px;
    }

    #rawPredictions pre {
      margin: 5px 0;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    #mewtResponse {
      font-size: 18px;
      min-height: 30px;
      font-weight: bold;
    }

    #loading {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      color: white;
      font-size: 24px;
      text-align: center;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body>
  <div id="container">
    <div id="loading">Loading models...<br/><small>This may take a moment</small></div>
    <video id="webcam" autoplay playsinline></video>
    <div id="predictionPanel" class="hidden">
      <h3>Analysis Results</h3>
      <div id="rawPredictions">
        <div><strong>Image:</strong> <span id="imageResults">Analyzing...</span></div>
        <div><strong>Audio:</strong> <span id="audioResults">Analyzing...</span></div>
      </div>
      <div id="mewtResponse">Ready</div>
    </div>
  </div>

  <script type="module">
    // Import MediaPipe libraries
    import {
      ImageClassifier,
      FilesetResolver as VisionFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8";
    
    import {
      AudioClassifier,
      FilesetResolver as AudioFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8";
    
    // Import Mewt module
    import Mewt from "./mewt.js";
    
    // Import VLM manager
    import { VLMChannel } from "./vlm-manager.js";
    
    // Import RN bridge
    import sendToRN from "./rn-bridge.js";

    // Get DOM elements
    const video = document.getElementById("webcam");
    const predictionPanel = document.getElementById("predictionPanel");
    const imageResultsElement = document.getElementById("imageResults");
    const audioResultsElement = document.getElementById("audioResults");
    const mewtResponseElement = document.getElementById("mewtResponse");
    const loadingElement = document.getElementById("loading");
    
    // Variables for classifiers
    let imageClassifier;
    let audioClassifier;
    let audioContext;
    let isRunning = false;
    
    // Storage for latest predictions
    let latestPredictions = {
      image: [],
      audio: []
    };
    
    // Initialize Mewt
    const mewt = new Mewt();
    
    // Initialize VLM Vision Channel
    const vlmVision = new VLMChannel('vision', {
      enabled: true,
      minInterval: 15000,  // 15秒
      maxPerMinute: 3      // 每分钟3次
    });
    
    // ==================== DeepMewt状态管理 ====================
    // 添加DeepMewt状态管理：维护全局状态，接收RN控制消息
    // 改动原因：支持RN动态切换DeepMewt模式，为后续audio+video增强做准备
    
    // 全局DeepMewt状态
    window.DEEP_MEWT_ENABLED = false;
    
    // RN消息监听器 - 接收deep_mewt_toggle消息
    window.addEventListener('message', function(event) {
      try {
        const data = JSON.parse(event.data);
        
        if (data.type === 'deep_mewt_toggle') {
          // 更新全局状态
          const previousState = window.DEEP_MEWT_ENABLED;
          window.DEEP_MEWT_ENABLED = data.enabled;
          
          console.log(`🧠 DeepMewt状态变更: ${previousState ? 'ON' : 'OFF'} → ${data.enabled ? 'ON' : 'OFF'}`);
          
          // 发送状态确认消息到RN
          sendToRN(
            `DeepMewt模式${data.enabled ? '已开启' : '已关闭'}`,
            'system',
            'idle',
            { 
              deepMewtEnabled: data.enabled,
              previousState: previousState,
              timestamp: Date.now()
            }
          );
        }
      } catch (e) {
        console.error('[RN Message] 解析错误:', e);
      }
    });
    
    // State stability mechanism
    let stableState = 'idle';
    let pendingState = 'idle';
    let stateChangeTime = 0;
    let lastStableState = 'idle';
    const STATE_DEBOUNCE_MS = 2000; // 2 seconds
    
    // Response deduplication
    let lastMewtResponse = '';
    
    // Helper: Get current video frame as base64
    const getCurrentFrame = () => {
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx = canvas.getContext('2d');
      ctx.drawImage(video, 0, 0);
      return canvas.toDataURL('image/jpeg', 0.8);
    };

    // Create ImageClassifier with proper error handling
    const createImageClassifier = async () => {
      try {
        console.log("Loading vision fileset...");
        const vision = await VisionFilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8/wasm"
        );
        
        console.log("Creating image classifier...");
        imageClassifier = await ImageClassifier.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite`
          },
          maxResults: 3,
          runningMode: "VIDEO"
        });
        
        console.log("Image classifier loaded successfully");
        return true;
      } catch (error) {
        console.error("Error loading image classifier:", error);
        loadingElement.innerHTML = "Error loading image classifier: " + error.message;
        return false;
      }
    };

    // Create AudioClassifier with proper error handling
    const createAudioClassifier = async () => {
      try {
        console.log("Loading audio fileset...");
        const audio = await AudioFilesetResolver.forAudioTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8/wasm"
        );
        
        console.log("Creating audio classifier...");
        audioClassifier = await AudioClassifier.createFromOptions(audio, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite"
          },
          maxResults: 3
        });
        
        console.log("Audio classifier loaded successfully");
        return true;
      } catch (error) {
        console.error("Error loading audio classifier:", error);
        loadingElement.innerHTML = "Error loading audio classifier: " + error.message;
        return false;
      }
    };

    // Load models sequentially to avoid conflicts
    const loadModels = async () => {
      loadingElement.innerHTML = "Loading image classifier...<br/><small>This may take a moment</small>";
      const imageSuccess = await createImageClassifier();
      
      if (!imageSuccess) return;
      
      loadingElement.innerHTML = "Loading audio classifier...<br/><small>This may take a moment</small>";
      const audioSuccess = await createAudioClassifier();
      
      if (!audioSuccess) return;
      
      // Hide loading indicator
      loadingElement.style.display = "none";
      predictionPanel.classList.remove("hidden");
      // Start classification
      startClassification();
    };

    // Update the display
    const updateDisplay = () => {
      // Update raw predictions
      if (latestPredictions.image.length > 0) {
        const topImages = latestPredictions.image.slice(0, 3);
        imageResultsElement.textContent = topImages.map(item => 
          `${item.categoryName} (${Math.round(item.score * 100)}%)`).join(', ');
      } else {
        imageResultsElement.textContent = "Analyzing...";
      }
      
      if (latestPredictions.audio.length > 0) {
        const topAudios = latestPredictions.audio.slice(0, 3);
        audioResultsElement.textContent = topAudios.map(item => 
          `${item.categoryName} (${Math.round(item.score * 100)}%)`).join(', ');
      } else {
        audioResultsElement.textContent = "Analyzing...";
      }
      
      // Get context for LRU trust
      const context = mewt.getFullContext();
      
      // Format predictions for Mewt
      const formattedPredictions = {
        image: latestPredictions.image.slice(0, 3).map(item => ({
          class: item.categoryName,
          score: item.score
        })),
        audio: latestPredictions.audio.slice(0, 3).map(item => ({
          class: item.categoryName,
          score: item.score
        }))
      };
      
      // Enhance visual detection with LRU trust
      let hasVisualCat = formattedPredictions.image.some(item => 
        item.class.toLowerCase().includes('cat') && item.score > 0.3
      );
      
      if (!hasVisualCat) {
        const recent10 = context.image_lru.recent(10);
        const catCount = recent10.filter(item => item.isCat).length;
        if (catCount > 0) {
          hasVisualCat = true; // Trust LRU history
        }
      }
      
      const hasAudioCat = formattedPredictions.audio.some(item => 
        (item.class.toLowerCase().includes('cat') || item.class.toLowerCase().includes('meow')) && 
        item.score > 0.2
      );
      
      // Determine raw state
      let rawState = 'idle';
      if (hasVisualCat && hasAudioCat) {
        rawState = 'cat_both';
      } else if (hasVisualCat) {
        rawState = 'cat_visual';
      } else if (hasAudioCat) {
        rawState = 'cat_audio';
      }
      
      // Apply state debouncing
      const now = Date.now();
      if (rawState !== pendingState) {
        pendingState = rawState;
        stateChangeTime = now;
      }
      
      // Only update stable state if pending state persists for 2 seconds
      if (now - stateChangeTime >= STATE_DEBOUNCE_MS) {
        stableState = pendingState;
        
        // Detect state change
        if (stableState !== lastStableState) {
          handleStateChange(stableState, lastStableState);
          lastStableState = stableState;
        }
      }
      
      // Get text from VLM (if locked) or use default
      const vlmText = vlmVision.getText();
      
      // Default state responses
      const stateResponses = {
        'idle': '观察中...',
        'cat_visual': '那里有只小猫',
        'cat_audio': '诶？我好像听到小猫叫了',
        'cat_both': '哦！是个小猫'
      };
      
      // Determine final text (VLM has priority)
      const finalText = vlmText || stateResponses[stableState];
      
      if (finalText && finalText !== lastMewtResponse) {
        mewtResponseElement.textContent = finalText;
        lastMewtResponse = finalText;
        
        // Send to RN with complete format
        const source = vlmText ? 'vlm' : 'state';
        const metadata = {
          hasCat: stableState !== 'idle',
          hasVisual: hasVisualCat,
          hasAudio: hasAudioCat
        };
        
        // Add VLM-specific metadata
        if (vlmText) {
          const vlmData = vlmVision.getLockData();
          metadata.confidence = vlmData?.confidence || 0.9;
          metadata.vlmLocked = true;
        }
        
        sendToRN(finalText, source, stableState, metadata);
      }
    };

    // Start classification
    const startClassification = async () => {
      if (isRunning) return;
      
      isRunning = true;
      
      // Start image classification
      try {
        // Try to get the rear camera by specifying constraints
        const constraints = {
          video: {
            facingMode: { ideal: "environment" } // Prefer rear camera
          }
        };
        
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        
        // Wait for video to actually start playing with valid frames
        video.addEventListener("playing", () => {
          setTimeout(() => {
            if (video.videoWidth > 0 && video.videoHeight > 0) {
              console.log(`✓ Video ready: ${video.videoWidth}x${video.videoHeight}`);
              predictWebcam();
            } else {
              console.error("Video playing but dimensions are 0");
            }
          }, 100);
        });
      } catch (err) {
        console.error("Error accessing webcam:", err);
        loadingElement.innerHTML = "Error accessing webcam: " + err.message;
        
        // Fallback to default camera if rear camera is not available
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ video: true });
          video.srcObject = stream;
          
          video.addEventListener("playing", () => {
            setTimeout(() => {
              if (video.videoWidth > 0 && video.videoHeight > 0) {
                console.log(`✓ Video ready (fallback): ${video.videoWidth}x${video.videoHeight}`);
                predictWebcam();
              } else {
                console.error("Video playing but dimensions are 0");
              }
            }, 100);
          });
        } catch (fallbackErr) {
          console.error("Error accessing any webcam:", fallbackErr);
          loadingElement.innerHTML = "Error accessing any webcam: " + fallbackErr.message;
        }
      }
      
      // Start audio classification
      startAudioClassification();
    };

    // Predict from webcam
    const predictWebcam = async () => {
      if (!imageClassifier || !video.videoWidth || !video.videoHeight) {
        window.requestAnimationFrame(predictWebcam);
        return;
      }
      
      try {
        const startTimeMs = performance.now();
        const classificationResult = imageClassifier.classifyForVideo(video, startTimeMs);
        const classifications = classificationResult.classifications;
        
        if (classifications.length > 0 && classifications[0].categories.length > 0) {
          // Store the latest image predictions
          latestPredictions.image = classifications[0].categories;
          updateDisplay();
        }
      } catch (error) {
        console.error("Image classification error:", error);
      }
      
      // Continue predicting
      window.requestAnimationFrame(predictWebcam);
    };

    // Start audio classification
    const startAudioClassification = async () => {
      const constraints = { audio: true };
      let stream;

      try {
        stream = await navigator.mediaDevices.getUserMedia(constraints);
      } catch (err) {
        console.error("Error accessing microphone:", err);
        loadingElement.innerHTML = "Error accessing microphone: " + err.message;
        return;
      }

      try {
        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);
        const scriptNode = audioContext.createScriptProcessor(16384, 1, 1);

        scriptNode.onaudioprocess = function (audioProcessingEvent) {
          try {
            if (!audioClassifier) return;
            
            const inputBuffer = audioProcessingEvent.inputBuffer;
            const inputData = inputBuffer.getChannelData(0);

            // Classify the audio
            const result = audioClassifier.classify(inputData);
            const categories = result[0].classifications[0].categories;

            // Store the latest audio predictions
            latestPredictions.audio = categories;
            updateDisplay();
          } catch (error) {
            console.error("Audio classification error:", error);
          }
        };

        source.connect(scriptNode);
        scriptNode.connect(audioContext.destination);
      } catch (error) {
        console.error("Error setting up audio classification:", error);
        loadingElement.innerHTML = "Error setting up audio classification: " + error.message;
      }
    };

    // Handle state changes - trigger VLM when appropriate
    const handleStateChange = async (newState, oldState) => {
      console.log(`[State Change] ${oldState} → ${newState}`);
      
      // Trigger VLM on transitions involving cat_visual or cat_both
      const hasVisual = newState === 'cat_visual' || newState === 'cat_both';
      const hadVisual = oldState === 'cat_visual' || oldState === 'cat_both';
      
      if (hasVisual && !hadVisual) {
        // Started seeing cat - call VLM
        console.log('[VLM Trigger] 检测到猫，调用VLM确认');
        const frame = getCurrentFrame();
        await vlmVision.analyze({ image: frame });
      } else if (!hasVisual && hadVisual) {
        // Lost cat - call VLM to confirm
        console.log('[VLM Trigger] 丢失猫，调用VLM确认');
        const frame = getCurrentFrame();
        await vlmVision.analyze({ image: frame });
      }
    };
    
    // Initialize when page loads
    window.addEventListener("DOMContentLoaded", loadModels);
  </script>
</body>
</html>
