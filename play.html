<!DOCTYPE html>
<!--
  Play Page - 音视频检测主页面
  
  功能：
  1. MediaPipe实时图像+音频检测
  2. Mewt四状态分类（idle/cat_visual/cat_audio/cat_both）
  3. VLM详细分析（Kimi视觉模型）
  4. RN通讯：发送检测结果文案
  
  最新改动 (2025-10-08):
  - 集成RN消息桥接（rn-bridge.js）
  - 发送完整消息格式：{type, text, source, state, timestamp, metadata}
  - 统一发送逻辑：状态变化发state，VLM返回发vlm
-->
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MediaPipe Audio & Image Classification with Mewt</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      background-color: #000;
      font-family: Arial, sans-serif;
    }

    #container {
      position: relative;
      width: 100vw;
      height: 100vh;
    }

    #webcam {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    #predictionPanel {
      position: absolute;
      bottom: 20px;
      left: 20px;
      background-color: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 15px;
      border-radius: 5px;
      z-index: 10;
      min-width: 300px;
      max-width: 500px;
    }

    #predictionPanel h3 {
      margin: 0 0 10px 0;
      font-size: 18px;
    }

    #rawPredictions {
      margin-bottom: 15px;
      font-size: 14px;
    }

    #rawPredictions pre {
      margin: 5px 0;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    #mewtResponse {
      font-size: 18px;
      min-height: 30px;
      font-weight: bold;
    }

    #loading {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      color: white;
      font-size: 24px;
      text-align: center;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body>
  <div id="container">
    <div id="loading">Loading models...<br/><small>This may take a moment</small></div>
    <video id="webcam" autoplay playsinline></video>
    <div id="predictionPanel" class="hidden">
      <h3>Analysis Results</h3>
      <div id="rawPredictions">
        <div><strong>Image:</strong> <span id="imageResults">Analyzing...</span></div>
        <div><strong>Audio:</strong> <span id="audioResults">Analyzing...</span></div>
      </div>
      <div id="mewtResponse">Ready</div>
    </div>
  </div>

  <script type="module">
    // Import MediaPipe libraries
    import {
      ImageClassifier,
      FilesetResolver as VisionFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8";
    
    import {
      AudioClassifier,
      FilesetResolver as AudioFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8";
    
    // Import Mewt module
    import Mewt from "./mewt.js";
    
    // Import VLM manager
    import { VLMChannel } from "./vlm-manager.js";
    
    // Import RN bridge
    import sendToRN from "./rn-bridge.js";

    // Get DOM elements
    const video = document.getElementById("webcam");
    const predictionPanel = document.getElementById("predictionPanel");
    const imageResultsElement = document.getElementById("imageResults");
    const audioResultsElement = document.getElementById("audioResults");
    const mewtResponseElement = document.getElementById("mewtResponse");
    const loadingElement = document.getElementById("loading");
    
    // Variables for classifiers
    let imageClassifier;
    let audioClassifier;
    let audioContext;
    let isRunning = false;
    
    // Storage for latest predictions
    let latestPredictions = {
      image: [],
      audio: []
    };
    
    // Initialize Mewt
    const mewt = new Mewt();
    
    // Initialize VLM Vision Channel
    const vlmVision = new VLMChannel('vision', {
      enabled: true,
      minInterval: 15000,  // 15秒
      maxPerMinute: 3      // 每分钟3次
    });
    
    // ==================== DeepMewt状态管理 ====================
    // 添加DeepMewt状态管理：维护全局状态，接收RN控制消息
    // 改动原因：支持RN动态切换DeepMewt模式，为后续audio+video增强做准备
    
    // 全局DeepMewt状态
    window.DEEP_MEWT_ENABLED = false;
    
    // RN消息监听器 - 接收deep_mewt_toggle消息
    window.addEventListener('message', function(event) {
      try {
        const data = JSON.parse(event.data);
        
        // 处理DeepMewt状态切换
        if (data.type === 'deep_mewt_toggle') {
          const previousState = window.DEEP_MEWT_ENABLED;
          window.DEEP_MEWT_ENABLED = data.enabled;
          
          console.log(`[DeepMewt] 状态切换: ${previousState} -> ${window.DEEP_MEWT_ENABLED}`);
          
          // 发送状态确认消息到RN
          sendToRN(
            `DeepMewt模式已${window.DEEP_MEWT_ENABLED ? '启用' : '禁用'}`,
            'deepmewt_status',
            stableState,
            {
              enabled: window.DEEP_MEWT_ENABLED,
              timestamp: Date.now(),
              previousState: previousState
            }
          );
        }
      } catch (e) {
        console.error('[RN Message] 解析错误:', e);
      }
    });
    
    // State stability mechanism
    let stableState = 'idle';
    let pendingState = 'idle';
    let stateChangeTime = 0;
    let lastStableState = 'idle';
    const STATE_DEBOUNCE_MS = 2000; // 2 seconds
    
    // Response deduplication
    let lastMewtResponse = '';
    
    // Helper: Get current video frame as base64
    const getCurrentFrame = () => {
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx = canvas.getContext('2d');
      ctx.drawImage(video, 0, 0);
      return canvas.toDataURL('image/jpeg', 0.8);
    };

    // Create ImageClassifier with proper error handling
    const createImageClassifier = async () => {
      try {
        console.log("Loading vision fileset...");
        const vision = await VisionFilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8/wasm"
        );
        
        console.log("Creating image classifier...");
        imageClassifier = await ImageClassifier.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite`
          },
          maxResults: 3,
          runningMode: "VIDEO"
        });
        
        console.log("Image classifier loaded successfully");
        return true;
      } catch (error) {
        console.error("Error loading image classifier:", error);
        loadingElement.innerHTML = "Error loading image classifier: " + error.message;
        return false;
      }
    };

    // Create AudioClassifier with proper error handling
    const createAudioClassifier = async () => {
      try {
        console.log("Loading audio fileset...");
        const audio = await AudioFilesetResolver.forAudioTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8/wasm"
        );
        
        console.log("Creating audio classifier...");
        audioClassifier = await AudioClassifier.createFromOptions(audio, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite"
          },
          maxResults: 3
        });
        
        console.log("Audio classifier loaded successfully");
        return true;
      } catch (error) {
        console.error("Error loading audio classifier:", error);
        loadingElement.innerHTML = "Error loading audio classifier: " + error.message;
        return false;
      }
    };

    // Start camera immediately
    const startCamera = async () => {
      try {
        const constraints = {
          video: { 
            facingMode: { ideal: "environment" },
            width: { ideal: 1080 },
            height: { ideal: 1920 },
            aspectRatio: { ideal: 9/16 }
          }
        };
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        
        return new Promise((resolve) => {
          video.addEventListener("playing", () => {
            setTimeout(() => {
              if (video.videoWidth > 0 && video.videoHeight > 0) {
                console.log(`✓ Camera ready: ${video.videoWidth}x${video.videoHeight}`);
                resolve(true);
              } else {
                console.error("Video playing but dimensions are 0");
                resolve(false);
              }
            }, 100);
          });
        });
      } catch (err) {
        console.error("Error accessing rear camera:", err);
        try {
          const fallbackStream = await navigator.mediaDevices.getUserMedia({ 
            video: { 
              width: { ideal: 1080 },
              height: { ideal: 1920 }
            }
          });
          video.srcObject = fallbackStream;
          
          return new Promise((resolve) => {
            video.addEventListener("playing", () => {
              setTimeout(() => {
                if (video.videoWidth > 0 && video.videoHeight > 0) {
                  console.log(`✓ Camera ready (fallback): ${video.videoWidth}x${video.videoHeight}`);
                  resolve(true);
                } else {
                  resolve(false);
                }
              }, 100);
            });
          });
        } catch (fallbackErr) {
          console.error("Error accessing any camera:", fallbackErr);
          loadingElement.innerHTML = "Error accessing camera: " + fallbackErr.message;
          return false;
        }
      }
    };

    // Load models in parallel
    const loadModels = async () => {
      loadingElement.innerHTML = "Loading image classifier...<br/><small>This may take a moment</small>";
      const imageSuccess = await createImageClassifier();
      
      if (!imageSuccess) return false;
      
      loadingElement.innerHTML = "Loading audio classifier...<br/><small>This may take a moment</small>";
      const audioSuccess = await createAudioClassifier();
      
      return audioSuccess;
    };

    // Initialize: camera + models in parallel
    const initialize = async () => {
      loadingElement.innerHTML = "Starting camera...<br/><small>Please allow camera access</small>";
      
      // Start camera and load models in parallel
      const [cameraReady, modelsReady] = await Promise.all([
        startCamera(),
        loadModels()
      ]);
      
      if (!cameraReady || !modelsReady) {
        if (!modelsReady) {
          loadingElement.innerHTML = "Error loading models";
        }
        return;
      }
      
      // Hide loading indicator and show panel
      loadingElement.style.display = "none";
      predictionPanel.classList.remove("hidden");
      
      // Start classification
      startClassification();
    };

    // Update the display
    const updateDisplay = () => {
      // Update raw predictions
      if (latestPredictions.image.length > 0) {
        const topImages = latestPredictions.image.slice(0, 3);
        imageResultsElement.textContent = topImages.map(item => 
          `${item.categoryName} (${Math.round(item.score * 100)}%)`).join(', ');
      } else {
        imageResultsElement.textContent = "Analyzing...";
      }
      
      if (latestPredictions.audio.length > 0) {
        const topAudios = latestPredictions.audio.slice(0, 3);
        audioResultsElement.textContent = topAudios.map(item => 
          `${item.categoryName} (${Math.round(item.score * 100)}%)`).join(', ');
      } else {
        audioResultsElement.textContent = "Analyzing...";
      }
      
      // Get context for LRU trust
      const context = mewt.getFullContext();
      
      // Format predictions for Mewt
      const formattedPredictions = {
        image: latestPredictions.image.slice(0, 3).map(item => ({
          class: item.categoryName,
          score: item.score
        })),
        audio: latestPredictions.audio.slice(0, 3).map(item => ({
          class: item.categoryName,
          score: item.score
        }))
      };
      
      // Enhance visual detection with LRU trust
      let hasVisualCat = formattedPredictions.image.some(item => 
        item.class.toLowerCase().includes('cat') && item.score > 0.3
      );
      
      if (!hasVisualCat) {
        const recent10 = context.image_lru.recent(10);
        const catCount = recent10.filter(item => item.isCat).length;
        if (catCount > 0) {
          hasVisualCat = true; // Trust LRU history
        }
      }
      
      const hasAudioCat = formattedPredictions.audio.some(item => 
        (item.class.toLowerCase().includes('cat') || item.class.toLowerCase().includes('meow')) && 
        item.score > 0.2
      );
      
      // Determine raw state
      let rawState = 'idle';
      if (hasVisualCat && hasAudioCat) {
        rawState = 'cat_both';
      } else if (hasVisualCat) {
        rawState = 'cat_visual';
      } else if (hasAudioCat) {
        rawState = 'cat_audio';
      }
      
      // Apply state debouncing
      const now = Date.now();
      if (rawState !== pendingState) {
        pendingState = rawState;
        stateChangeTime = now;
      }
      
      // Only update stable state if pending state persists for 2 seconds
      if (now - stateChangeTime >= STATE_DEBOUNCE_MS) {
        stableState = pendingState;
        
        // Detect state change
        if (stableState !== lastStableState) {
          handleStateChange(stableState, lastStableState);
          lastStableState = stableState;
        }
      }
      
      // Get text from VLM (if locked) or use default
      const vlmText = vlmVision.getText();
      
      // Default state responses
      const stateResponses = {
        'idle': '观察中...',
        'cat_visual': '那里有只小猫',
        'cat_audio': '诶？我好像听到小猫叫了',
        'cat_both': '哦！是个小猫'
      };
      
      // Determine final text (VLM has priority)
      const finalText = vlmText || stateResponses[stableState];
      
      if (finalText && finalText !== lastMewtResponse) {
        mewtResponseElement.textContent = finalText;
        lastMewtResponse = finalText;
        
        // Send to RN with complete format
        const source = vlmText ? 'vlm' : 'state';
        const metadata = {
          hasCat: stableState !== 'idle',
          hasVisual: hasVisualCat,
          hasAudio: hasAudioCat
        };
        
        // Add VLM-specific metadata
        if (vlmText) {
          const vlmData = vlmVision.getLockData();
          metadata.confidence = vlmData?.confidence || 0.9;
          metadata.vlmLocked = true;
        }
        
        sendToRN(finalText, source, stableState, metadata);
      }
    };

    // Frame rate control for image prediction
    let lastPredictionTime = 0;
    const PREDICTION_INTERVAL = 500; // 500ms = 2 frames per second
    
    // Start classification (camera already started in initialize)
    const startClassification = async () => {
      if (isRunning) return;
      isRunning = true;
      
      // Start image classification (video already playing)
      predictWebcam();
      
      // Start audio classification
      startAudioClassification();
    };

    // Predict from webcam
    const predictWebcam = async () => {
      if (!imageClassifier || !video.videoWidth || !video.videoHeight) {
        setTimeout(predictWebcam, PREDICTION_INTERVAL);
        return;
      }
      
      try {
        const now = performance.now();
        
        // Only predict if enough time has passed
        if (now - lastPredictionTime >= PREDICTION_INTERVAL) {
          lastPredictionTime = now;
          
          const classificationResult = imageClassifier.classifyForVideo(video, now);
          const classifications = classificationResult.classifications;
          
          if (classifications.length > 0 && classifications[0].categories.length > 0) {
            // Store the latest image predictions
            latestPredictions.image = classifications[0].categories;
            updateDisplay();
          }
        }
      } catch (error) {
        console.error("Image classification error:", error);
      }
      
      // Continue predicting with controlled frame rate
      setTimeout(predictWebcam, PREDICTION_INTERVAL);
    };

    // Start audio classification
    const startAudioClassification = async () => {
      const constraints = { audio: true };
      let stream;

      try {
        stream = await navigator.mediaDevices.getUserMedia(constraints);
      } catch (err) {
        console.error("Error accessing microphone:", err);
        loadingElement.innerHTML = "Error accessing microphone: " + err.message;
        return;
      }

      try {
        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);
        const scriptNode = audioContext.createScriptProcessor(16384, 1, 1);

        scriptNode.onaudioprocess = function (audioProcessingEvent) {
          try {
            if (!audioClassifier) return;
            
            const inputBuffer = audioProcessingEvent.inputBuffer;
            const inputData = inputBuffer.getChannelData(0);

            // Classify the audio
            const result = audioClassifier.classify(inputData);
            const categories = result[0].classifications[0].categories;

            // Store the latest audio predictions
            latestPredictions.audio = categories;
            updateDisplay();
          } catch (error) {
            console.error("Audio classification error:", error);
          }
        };

        source.connect(scriptNode);
        scriptNode.connect(audioContext.destination);
      } catch (error) {
        console.error("Error setting up audio classification:", error);
        loadingElement.innerHTML = "Error setting up audio classification: " + error.message;
      }
    };

    // Handle state changes - trigger VLM when appropriate
    const handleStateChange = async (newState, oldState) => {
      console.log(`[State Change] ${oldState} → ${newState}`);
      
      // Trigger VLM on transitions involving cat_visual or cat_both
      const hasVisual = newState === 'cat_visual' || newState === 'cat_both';
      const hadVisual = oldState === 'cat_visual' || oldState === 'cat_both';
      
      if (hasVisual && !hadVisual) {
        // Started seeing cat - call VLM
        console.log('[VLM Trigger] 检测到猫，调用VLM确认');
        const frame = getCurrentFrame();
        await vlmVision.analyze({ image: frame });
      } else if (!hasVisual && hadVisual) {
        // Lost cat - call VLM to confirm
        console.log('[VLM Trigger] 丢失猫，调用VLM确认');
        const frame = getCurrentFrame();
        await vlmVision.analyze({ image: frame });
      }
    };
    
    // Initialize when page loads
    window.addEventListener("DOMContentLoaded", initialize);
  </script>
</body>
</html>
