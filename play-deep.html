<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MediaPipe Audio & Image Classification with Quick Response</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      background-color: #000;
      font-family: Arial, sans-serif;
    }

    #container {
      position: relative;
      width: 100vw;
      height: 100vh;
    }

    #webcam {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    #predictionPanel {
      position: absolute;
      bottom: 20px;
      left: 20px;
      background-color: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 15px;
      border-radius: 5px;
      z-index: 10;
      min-width: 300px;
      max-width: 500px;
    }

    #audioFeatures {
      margin-top: 10px;
      font-size: 12px;
    }

    #audioFeatures div {
      margin: 3px 0;
    }

    #predictionPanel h3 {
      margin: 0 0 10px 0;
      font-size: 18px;
    }

    #rawPredictions {
      margin-bottom: 15px;
      font-size: 14px;
    }

    #rawPredictions pre {
      margin: 5px 0;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    #quickResponse {
      font-size: 18px;
      min-height: 30px;
      font-weight: bold;
    }

    #loading {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      color: white;
      font-size: 24px;
      text-align: center;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body>
  <div id="container">
    <div id="loading">Loading models...<br/><small>This may take a moment</small></div>
    <video id="webcam" autoplay playsinline></video>
    <div id="predictionPanel" class="hidden">
      <h3>Analysis Results</h3>
      <div id="rawPredictions">
        <div><strong>Image:</strong> <span id="imageResults">Analyzing...</span></div>
        <div><strong>Audio:</strong> <span id="audioResults">Analyzing...</span></div>
      </div>
      <div id="audioFeatures">
        <div><strong>Zero Crossing Rate:</strong> <span id="zeroCrossingRate">N/A</span></div>
        <div><strong>Spectral Centroid:</strong> <span id="spectralCentroid">N/A</span></div>
        <div><strong>Spectral Rolloff:</strong> <span id="spectralRolloff">N/A</span></div>
        <div><strong>Energy:</strong> <span id="energy">N/A</span></div>
        <div><strong>RMS:</strong> <span id="rms">N/A</span></div>
        <div><strong>Top Categories:</strong> <span id="topCategories">N/A</span></div>
        <div><strong>Max Score:</strong> <span id="maxScore">N/A</span></div>
      </div>
      <div id="quickResponse">Ready</div>
    </div>
  </div>

  <script type="module">
    // Import MediaPipe libraries
    import {
      ImageClassifier,
      FilesetResolver as VisionFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8";
    
    import {
      AudioClassifier,
      FilesetResolver as AudioFilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8";

  // Import audio feature extraction functions
  import { calculateZCR, calculateSpectralCentroid, calculateSpectralRolloff, calculateEnergy, calculateRMS } from './features.js';
  
  // Import new context management system
  import Mewt from './mewt.js';

    // Get DOM elements
    const video = document.getElementById("webcam");
    const predictionPanel = document.getElementById("predictionPanel");
    const imageResultsElement = document.getElementById("imageResults");
    const audioResultsElement = document.getElementById("audioResults");
    const quickResponseElement = document.getElementById("quickResponse");
    const loadingElement = document.getElementById("loading");
    const zeroCrossingRateElement = document.getElementById("zeroCrossingRate");
    const spectralCentroidElement = document.getElementById("spectralCentroid");
    const spectralRolloffElement = document.getElementById("spectralRolloff");
    const energyElement = document.getElementById("energy");
    const rmsElement = document.getElementById("rms");
    const topCategoriesElement = document.getElementById("topCategories");
    const maxScoreElement = document.getElementById("maxScore");
    
    // Variables for classifiers
    let imageClassifier;
    let audioClassifier;
    let audioContext;
    let isRunning = false;
    
    // New context management system
    let mewtContext = new Mewt();
    
    // Storage for latest predictions
    let latestPredictions = {
      image: [],
      audio: [],
      audioBuffer: null
    };

    // Create ImageClassifier with proper error handling
    const createImageClassifier = async () => {
      try {
        console.log("Loading vision fileset...");
        const vision = await VisionFilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.8/wasm"
        );
        
        console.log("Creating image classifier...");
        imageClassifier = await ImageClassifier.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite`
          },
          maxResults: 3,
          runningMode: "VIDEO"
        });
        
        console.log("Image classifier loaded successfully");
        return true;
      } catch (error) {
        console.error("Error loading image classifier:", error);
        loadingElement.innerHTML = "Error loading image classifier: " + error.message;
        return false;
      }
    };

    // Create AudioClassifier with proper error handling
    const createAudioClassifier = async () => {
      try {
        console.log("Loading audio fileset...");
        const audio = await AudioFilesetResolver.forAudioTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@0.10.8/wasm"
        );
        
        console.log("Creating audio classifier...");
        audioClassifier = await AudioClassifier.createFromOptions(audio, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite"
          },
          maxResults: 5, // Increase max results to get more categories
          scoreThreshold: 0.1 // Lower threshold to detect more subtle sounds
        });
        
        console.log("Audio classifier loaded successfully");
        return true;
      } catch (error) {
        console.error("Error loading audio classifier:", error);
        loadingElement.innerHTML = "Error loading audio classifier: " + error.message;
        return false;
      }
    };

    // Load models sequentially to avoid conflicts
    const loadModels = async () => {
      loadingElement.innerHTML = "Loading image classifier...<br/><small>This may take a moment</small>";
      const imageSuccess = await createImageClassifier();
      
      if (!imageSuccess) return;
      
      loadingElement.innerHTML = "Loading audio classifier...<br/><small>This may take a moment</small>";
      const audioSuccess = await createAudioClassifier();
      
      if (!audioSuccess) return;
      
      // Hide loading indicator
      loadingElement.style.display = "none";
      predictionPanel.classList.remove("hidden");
      // Start classification
      startClassification();
    };

    // Update the display
    const updateDisplay = () => {
      // Update raw predictions
      if (latestPredictions.image.length > 0) {
        const topImages = latestPredictions.image.slice(0, 3);
        imageResultsElement.textContent = topImages.map(item => 
          `${item.categoryName} (${Math.round(item.score * 100)}%)`).join(', ');
      } else {
        imageResultsElement.textContent = "Analyzing...";
      }
      
      if (latestPredictions.audio.length > 0) {
        const topAudios = latestPredictions.audio.slice(0, 3);
        audioResultsElement.textContent = topAudios.map(item => 
          `${item.categoryName} (${Math.round(item.score * 100)}%)`).join(', ');
      } else {
        audioResultsElement.textContent = "Analyzing...";
      }
      
      // Update audio features
      if (latestPredictions.audioBuffer) {
        // Calculate and display audio features
        const zcr = calculateZCR(latestPredictions.audioBuffer);
        const spectralCentroid = calculateSpectralCentroid(latestPredictions.audioBuffer);
        const spectralRolloff = calculateSpectralRolloff(latestPredictions.audioBuffer);
        const energy = calculateEnergy(latestPredictions.audioBuffer);
        const rms = calculateRMS(latestPredictions.audioBuffer);
        
        zeroCrossingRateElement.textContent = zcr.toFixed(4);
        spectralCentroidElement.textContent = spectralCentroid.toFixed(2);
        spectralRolloffElement.textContent = spectralRolloff.toFixed(4);
        energyElement.textContent = energy.toFixed(6);
        rmsElement.textContent = rms.toFixed(6);
        
        // Display top categories and max score
        if (latestPredictions.audio.length > 0) {
          const topCategories = latestPredictions.audio
            .slice(0, 3)
            .map(item => item.categoryName)
            .join(', ');
          topCategoriesElement.textContent = topCategories;
          
          const maxScore = Math.max(...latestPredictions.audio.map(item => item.score));
          maxScoreElement.textContent = maxScore.toFixed(2);
        } else {
          topCategoriesElement.textContent = "N/A";
          maxScoreElement.textContent = "N/A";
        }
      } else {
        // Reset all feature displays
        zeroCrossingRateElement.textContent = "N/A";
        spectralCentroidElement.textContent = "N/A";
        spectralRolloffElement.textContent = "N/A";
        energyElement.textContent = "N/A";
        rmsElement.textContent = "N/A";
        topCategoriesElement.textContent = "N/A";
        maxScoreElement.textContent = "N/A";
      }
      
      // Generate quick response
      generateQuickResponse();
    };

    // Check if any of the top 3 results contain cat-related categories
    const hasCatInTop3 = (results) => {
      return results.slice(0, 3).some(item => 
        item.categoryName.toLowerCase().includes('cat') && item.score > 0.3);
    };

    // Check if any of the top 3 results contain cat sound categories
    const hasCatSoundInTop3 = (results) => {
      return results.slice(0, 3).some(item => 
        (item.categoryName.toLowerCase().includes('cat') || 
         item.categoryName.toLowerCase().includes('meow')) && item.score > 0.2);
    };

    // Generate quick response using the new context management system
    const generateQuickResponse = () => {
      try {
        // 获取上下文管理系统的状态
        const context = mewtContext.getFullContext();
        
        // 如果没有情绪响应覆盖，使用基础状态响应
        const hasVisual = mewtContext.hasVisualCat();
        const hasAudio = mewtContext.hasCatSound();
        const currentState = mewtContext.determineState(hasVisual, hasAudio);
        
        // 生成基于状态的响应
        const stateResponse = mewtContext.stateResponses[currentState];
        
        // 如果当前文本不是情绪响应，更新为状态响应
        if (quickResponseElement.style.color !== 'rgb(255, 107, 107)') {
          quickResponseElement.textContent = stateResponse;
        }
        
        // 显示调试信息
        console.log('🎯 当前状态:', currentState, '| 关注猫咪:', context.is_now_focusing_cat);
        
      } catch (error) {
        console.error('生成快速响应时出错:', error);
        quickResponseElement.textContent = "观察中...";
      }
    };

    // Start classification
    const startClassification = async () => {
      if (isRunning) return;
      
      isRunning = true;
      
      // Start image classification
      try {
        // Try to get the rear camera by specifying constraints
        const constraints = {
          video: {
            facingMode: { ideal: "environment" } // Prefer rear camera
          }
        };
        
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        video.addEventListener("loadeddata", predictWebcam);
      } catch (err) {
        console.error("Error accessing webcam:", err);
        loadingElement.innerHTML = "Error accessing webcam: " + err.message;
        
        // Fallback to default camera if rear camera is not available
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ video: true });
          video.srcObject = stream;
          video.addEventListener("loadeddata", predictWebcam);
        } catch (fallbackErr) {
          console.error("Error accessing any webcam:", fallbackErr);
          loadingElement.innerHTML = "Error accessing any webcam: " + fallbackErr.message;
        }
      }
      
      // Start audio classification
      startAudioClassification();
    };

    // Predict from webcam - 新的上下文管理系统
    const predictWebcam = async () => {
      if (!imageClassifier || !video.videoWidth || !video.videoHeight) {
        window.requestAnimationFrame(predictWebcam);
        return;
      }
      
      try {
        const startTimeMs = performance.now();
        const classificationResult = imageClassifier.classifyForVideo(video, startTimeMs);
        const classifications = classificationResult.classifications;
        
        if (classifications.length > 0 && classifications[0].categories.length > 0) {
          // 转换MediaPipe格式到新上下文管理系统格式
          const imageResults = classifications[0].categories.map(category => ({
            class: category.categoryName,
            score: category.score
          }));
          
          // 添加到上下文管理系统
          mewtContext.addImageResult(imageResults);
          
          // 保留用于显示
          latestPredictions.image = classifications[0].categories;
          updateDisplay();
        }
      } catch (error) {
        console.error("Image classification error:", error);
      }
      
      // Continue predicting
      window.requestAnimationFrame(predictWebcam);
    };

    // Start audio classification
    const startAudioClassification = async () => {
      const constraints = { audio: true };
      let stream;

      try {
        stream = await navigator.mediaDevices.getUserMedia(constraints);
      } catch (err) {
        console.error("Error accessing microphone:", err);
        loadingElement.innerHTML = "Error accessing microphone: " + err.message;
        return;
      }

      try {
        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);
        const scriptNode = audioContext.createScriptProcessor(16384, 1, 1);

        scriptNode.onaudioprocess = function (audioProcessingEvent) {
          try {
            if (!audioClassifier) return;
            
            const inputBuffer = audioProcessingEvent.inputBuffer;
            const inputData = inputBuffer.getChannelData(0);

            // Save audio buffer for feature extraction
            latestPredictions.audioBuffer = inputData;

            // Classify the audio
            const result = audioClassifier.classify(inputData);
            const categories = result[0].classifications[0].categories;

            // 转换MediaPipe格式到新上下文管理系统格式
            const audioResults = categories.map(category => ({
              class: category.categoryName,
              score: category.score
            }));

            // 添加到上下文管理系统，可能触发情绪分析
            const emotionResponse = mewtContext.addAudioResult(audioResults, inputData);
            
            // 如果有情绪响应，显示在界面上
            if (emotionResponse) {
              console.log('🎭 情绪分析触发:', emotionResponse);
              quickResponseElement.textContent = emotionResponse.text;
              quickResponseElement.style.color = '#ff6b6b'; // 高亮显示情绪
              
              // 3秒后恢复正常颜色
              setTimeout(() => {
                quickResponseElement.style.color = 'white';
              }, 3000);
            }

            // Store the latest audio predictions for display
            latestPredictions.audio = categories;
            updateDisplay();
          } catch (error) {
            console.error("Audio classification error:", error);
          }
        };

        source.connect(scriptNode);
        scriptNode.connect(audioContext.destination);
      } catch (error) {
        console.error("Error setting up audio classification:", error);
        loadingElement.innerHTML = "Error setting up audio classification: " + error.message;
      }
    };

    // Initialize when page loads
    window.addEventListener("DOMContentLoaded", loadModels);
  </script>
</body>
</html>
